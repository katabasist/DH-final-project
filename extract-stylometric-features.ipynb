{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ea6623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92cbaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39e0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def download_nltk_resources():\n",
    "    resources = {\n",
    "        'tokenizers': ['punkt'],\n",
    "        'taggers': ['averaged_perceptron_tagger'],\n",
    "        'corpora': ['stopwords']\n",
    "    }\n",
    "    for resource_type, packages in resources.items():\n",
    "        for package in packages:\n",
    "            try:\n",
    "                nltk.data.find(f'{resource_type}/{package}')\n",
    "            except LookupError:\n",
    "                nltk.download(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc9ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted and saved to tp-stylometry.csv!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "##import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def extract_features(text):\n",
    "    \"\"\"Extracts stylometric features from a single episode's text.\"\"\"\n",
    "    # Tokenize words and sentences\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Filter out punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_clean = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Part-of-Speech tagging\n",
    "    pos_tags = pos_tag(words_clean)\n",
    "    pos_counts = Counter(tag for (word, tag) in pos_tags)\n",
    "    \n",
    "    # Feature calculations\n",
    "    features = {\n",
    "        # Lexical features\n",
    "        'word_count': len(words),\n",
    "        'unique_words': len(set(words)),\n",
    "        'lexical_diversity': len(set(words)) / len(words) if words else 0,  # Type-Token Ratio (TTR)\n",
    "        'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "        'hapax_legomena': len([word for word, count in Counter(words).items() if count == 1]),\n",
    "        \n",
    "        # Syntactic features\n",
    "        'avg_sentence_length': np.mean([len(word_tokenize(sent)) for sent in sentences]) if sentences else 0,\n",
    "        'prop_nouns': pos_counts.get('NN', 0) + pos_counts.get('NNS', 0),\n",
    "        'prop_verbs': pos_counts.get('VB', 0) + pos_counts.get('VBD', 0) + pos_counts.get('VBG', 0) + pos_counts.get('VBN', 0) + pos_counts.get('VBP', 0) + pos_counts.get('VBZ', 0),\n",
    "        'prop_adjectives': pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + pos_counts.get('JJS', 0),\n",
    "        'prop_adverbs': pos_counts.get('RB', 0) + pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0),\n",
    "        'prop_pronouns': pos_counts.get('PRP', 0) + pos_counts.get('PRP$', 0),\n",
    "        \n",
    "        # Stylistic features\n",
    "        'exclamations': text.count('!'),\n",
    "        'questions': text.count('?'),\n",
    "        'ellipses': text.count('...'),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def process_episodes(input_dir):\n",
    "    \"\"\"Processes all .txt files in a directory and returns a DataFrame of features.\"\"\"\n",
    "    features_list = []\n",
    "    filenames = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(input_dir)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(input_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                features = extract_features(text)\n",
    "                features_list.append(features)\n",
    "                filenames.append(filename)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(features_list, index=filenames)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "input_directory = \"cleaner-text\"  # Folder with cleaned .txt files\n",
    "output_csv = \"tp-stylometry.csv\"\n",
    "\n",
    "# Extract features and save to CSV\n",
    "df_features = process_episodes(input_directory)\n",
    "df_features.to_csv(output_csv)\n",
    "print(f\"Features extracted and saved to {output_csv}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
